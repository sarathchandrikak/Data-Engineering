# Data Pipelines

▸✸ Data Pipeline is a series of connected process where output of one process is input of the next.\
▸✸ They move or modify data from one place or form to other.\
▸✸ Data flow in form of data packets.\

# Data Pipeline Performance

▸✸ Performance is measured by latency, throughput.\
▸✸ Latency refers to the total time taken for a single packet of data to pass within the pipeline and overall latency is limited by the slowest process in the pipeline.\
▸✸ Throughput refers to how much data can be fed through the pipeline per unit of time. Prcessing larger packets per unit time increases throughput.\

# Use Cases 

▸✸ Backing up files\
▸✸ Integrating disparate raw data sources into data lake\
▸✸ Moving transactional records to a data warehouse\
▸✸ Streaming data from IoT devices to dashboards\
▸✸ Preparing raw data fro machine learning development or production\
▸✸ Messaging systems such as email, SMS, video meetings\

# Data Pipeline Stages

▸✸ Data Extraction\
▸✸ Data Ingestion\
▸✸ Transformation Changes\
▸✸ Loading data into destination facility\
▸✸ Scheduling or triggering the job to run\
▸✸ Monitoring Entire workflow\
▸✸ Maintenance and Optimization of the pipeline\

# Pipeline Monitoring Considerations 

▸✸ Latency\ 
▸✸ Throughput\ 
▸✸ Warings, errors, failures\
▸✸ Utilization rate\
▸✸ Logging and alerting systems \

Unbalanced loads can be handled by introducing parallelization and I/O buffers can help mitigate bottlenecks.
